<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.2">Jekyll</generator><link href="https://diehard073055.github.io//sturdy-carnival/feed.xml" rel="self" type="application/atom+xml" /><link href="https://diehard073055.github.io//sturdy-carnival/" rel="alternate" type="text/html" /><updated>2018-06-14T01:23:43+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/</id><title type="html">AWS Developer Associate Notes</title><subtitle>Blog to keep track of what i have been studying for AWS Developer Associate Course. Also will be referrence to study before the exam.</subtitle><entry><title type="html">Snowball</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/Snowball.html" rel="alternate" type="text/html" title="Snowball" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/Snowball</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/Snowball.html">&lt;p&gt;AWS Import/Export Disk accelerates moving large amounts data into and out of the AWS cloud using portable storage devices for transport. AWS Import/Export Disk transfers your data directly onto and off of storage devices using Amazon’s high-speed internal network and bypassing the internet.&lt;/p&gt;

&lt;h3 id=&quot;types-of-snowballs&quot;&gt;Types of Snowballs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Snowball&lt;/li&gt;
  &lt;li&gt;Snowball Edge&lt;/li&gt;
  &lt;li&gt;Snowmobile&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;snowball&quot;&gt;Snowball&lt;/h3&gt;
&lt;p&gt;Snowball is petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed internet. 80 TB snowball in all regions. Snowball uses multiple layers of security designed to protect your data including tamper-resistent enclosures, 256-bit encryption, and an industry standard Trusted Platform Module (TPM) designed to ensure both security and full chain-of-custody of your data. Once the data transfer job has been processed and verified, AWS performs a software erasure of Snowball appliance.&lt;/p&gt;

&lt;h3 id=&quot;snowball-edge&quot;&gt;Snowball Edge&lt;/h3&gt;
&lt;p&gt;AWS Snowball Edge is a 100TB data transfer device with on-board storage and compute capabilities. You can use Snowball Edge to move large amounts of data into and out of AWS, as a temporary storage tier for large local datasets, or to support local workloads in remote or offline locations.&lt;/p&gt;

&lt;p&gt;Snowball Edge connects your existing applications and infrastructure using standard storage interfaces, streamlinning the data transfer process and minimizing setup and integration. Snowball Edge can cluster together to form a local storage tier and process your data on-premises, helping ensure your applications continue to run even when they are not able to access the cloud.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Its not just storage capacity its compute capacity in a box as well&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;snowmobile&quot;&gt;Snowmobile&lt;/h3&gt;
&lt;p&gt;AWS Snowmobile is an Exabyte-scale data transfer service use to move extremely large amounts of data to AWS. You can transfer upto 100PB per Snowmobile. a 45-long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even complete data center migration. Transfering data with Snowmobile is secure, fast and cost effective.&lt;/p&gt;

&lt;h3 id=&quot;exam-tips&quot;&gt;Exam tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Understand what snowball is&lt;/li&gt;
  &lt;li&gt;Understand what Import Export is&lt;/li&gt;
  &lt;li&gt;Snowball Can do
    &lt;ul&gt;
      &lt;li&gt;Import to S3&lt;/li&gt;
      &lt;li&gt;Export from S3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="transfer" /><summary type="html">AWS Import/Export Disk accelerates moving large amounts data into and out of the AWS cloud using portable storage devices for transport. AWS Import/Export Disk transfers your data directly onto and off of storage devices using Amazon’s high-speed internal network and bypassing the internet.</summary></entry><entry><title type="html">S3 Transfer Acceleration</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Transfer-Acceleration.html" rel="alternate" type="text/html" title="S3 Transfer Acceleration" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Transfer-Acceleration</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Transfer-Acceleration.html">&lt;p&gt;S3 Transfer Acceleration utilises the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that to S3. You will get a distinct URL to upload to.&lt;/p&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="transfer" /><category term="s3" /><category term="transferacceleration" /><summary type="html">S3 Transfer Acceleration utilises the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that to S3. You will get a distinct URL to upload to.</summary></entry><entry><title type="html">S3 Storage Summary</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Storage-Summary.html" rel="alternate" type="text/html" title="S3 Storage Summary" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Storage-Summary</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Storage-Summary.html">&lt;h3 id=&quot;s3---exam-tips&quot;&gt;S3 - Exam Tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;S3 is an Object based storage, its a place where you upload and download files.
    &lt;ul&gt;
      &lt;li&gt;You cannot run an os, or run a database off of S3.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Files can be of size 0 bytes all the way to 5TB.&lt;/li&gt;
  &lt;li&gt;There is unlimited storage.&lt;/li&gt;
  &lt;li&gt;Files are stored in Buckets.&lt;/li&gt;
  &lt;li&gt;S3 is a universal namespace, all bucket names shall be unique globally.&lt;/li&gt;
  &lt;li&gt;S3 bucket names are like &lt;code class=&quot;highlighter-rouge&quot;&gt;https://s3-[region].amazonaws.com/[bucketname]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Read after Write Consistency for PUTS of new objects.&lt;/li&gt;
  &lt;li&gt;Eventual Consistencry for Overwrite PUTS and DELETES. ( takes some time to propergate )&lt;/li&gt;
  &lt;li&gt;S3 Storage Classes / Tiers
    &lt;ul&gt;
      &lt;li&gt;Standard S3
        &lt;ul&gt;
          &lt;li&gt;reliability of 99.999999999%&lt;/li&gt;
          &lt;li&gt;availability of 99.99%&lt;/li&gt;
          &lt;li&gt;Immedietly available and frequently accessed&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;S3 Infrequent Access ( IA )
        &lt;ul&gt;
          &lt;li&gt;availablity of 99.99%&lt;/li&gt;
          &lt;li&gt;reliability of 99.999999999%&lt;/li&gt;
          &lt;li&gt;Immedietly available infrequently accessed.&lt;/li&gt;
          &lt;li&gt;Charged for retreval&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;S3 Reduced Redundency
        &lt;ul&gt;
          &lt;li&gt;reliability and availability of 99.99%&lt;/li&gt;
          &lt;li&gt;Used for reproducable content ( such as thumbnails etc ).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Glacier
        &lt;ul&gt;
          &lt;li&gt;Archived data, where you have to wait 3 - 5 hour waiting period.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Core fundementals of S3
    &lt;ul&gt;
      &lt;li&gt;Key ( name )&lt;/li&gt;
      &lt;li&gt;Value ( data )&lt;/li&gt;
      &lt;li&gt;Version ID&lt;/li&gt;
      &lt;li&gt;Metadata ( data about data )&lt;/li&gt;
      &lt;li&gt;Access control lists&lt;/li&gt;
      &lt;li&gt;Object based stroage only ( flat files )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 versioning
    &lt;ul&gt;
      &lt;li&gt;Stores all versions of an object ( including all writes and even if you delete an object )&lt;/li&gt;
      &lt;li&gt;Great backup tool&lt;/li&gt;
      &lt;li&gt;You pay for storage for all the versions of the object.&lt;/li&gt;
      &lt;li&gt;Once enabled versioning cannot be disabled, it can only be suspended.&lt;/li&gt;
      &lt;li&gt;Integrates with lifecycle rules.&lt;/li&gt;
      &lt;li&gt;Versionings MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.&lt;/li&gt;
      &lt;li&gt;Cross region replication requires versioning enabled on both the source bucket and destination bucket.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Lifecycle Management
    &lt;ul&gt;
      &lt;li&gt;Can be used in conjunction with versioning.&lt;/li&gt;
      &lt;li&gt;Can be applied to current versions and previous versions.&lt;/li&gt;
      &lt;li&gt;Following actions can now be done.
        &lt;ul&gt;
          &lt;li&gt;Transition to infrequenct access after a minimum of 30 days since creation date ( files need to be minimum 128kb )&lt;/li&gt;
          &lt;li&gt;Archived to glacier storage class ( 30 days after IA, if relevant ).&lt;/li&gt;
          &lt;li&gt;Permenantly Delete Objects&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CloudFront
    &lt;ul&gt;
      &lt;li&gt;Edge Locations - This is the location where content will be cached.&lt;/li&gt;
      &lt;li&gt;Origin - This is the origin of all the files that the CDN will distribute. This can be either an S3 bucket, an EC2 instance, an Elastic Load balancer or Route53.&lt;/li&gt;
      &lt;li&gt;Distribution - This is the name given to the CDN with a collections of Edge Locations.
        &lt;ul&gt;
          &lt;li&gt;Web Distribution - Typically used for websites&lt;/li&gt;
          &lt;li&gt;RTMP - Used for Media Streaming&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Edge locations are not just readonly, you can write to them as well. ( Upload files to S3 ).&lt;/li&gt;
      &lt;li&gt;Object are cached for the life of the TTL ( time to live ) ( default ttl 24hours )&lt;/li&gt;
      &lt;li&gt;You can clear cached objects, but you will be charged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Securing your buckets.
    &lt;ul&gt;
      &lt;li&gt;By default, all newly created buckets are &lt;strong&gt;PRIVATE&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;You can setup access control to your buckets using;
        &lt;ul&gt;
          &lt;li&gt;Bucket policies.&lt;/li&gt;
          &lt;li&gt;Access Control Lists.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;S3 buckets can be configured to create access logs which logs all the access requests made to the bucke. These logs can be stored in another bucket.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encryption
    &lt;ul&gt;
      &lt;li&gt;In-transit Encryption
        &lt;ul&gt;
          &lt;li&gt;SSL/TLS (HTTPS)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;In rest
        &lt;ul&gt;
          &lt;li&gt;Server Side Encryption
            &lt;ul&gt;
              &lt;li&gt;S3 Managed Keys SSE-S3&lt;/li&gt;
              &lt;li&gt;AWS Managed Keys SSE-KMS
                &lt;ul&gt;
                  &lt;li&gt;Allows you to monitor who decrypted which file when. (Audit trail)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Customer provided Keys SSE-C&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Client Side Encryption
            &lt;ul&gt;
              &lt;li&gt;Client uploads encrypted files.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Storage Gateway
    &lt;ul&gt;
      &lt;li&gt;File Gateway - for flatfiles,  stored directly on S3.&lt;/li&gt;
      &lt;li&gt;Volume Gateway (ISCI) - block based storage
        &lt;ul&gt;
          &lt;li&gt;Stored Volumes - your on premises data is asynchronously backed up to S3.&lt;/li&gt;
          &lt;li&gt;Cached Volumes - frequently accessed data is cached on premises from S3.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Gateway VIrtual Taped Library (VTL)
        &lt;ul&gt;
          &lt;li&gt;Used for backing up your data, from application like Netbackup, Backup Exec, Veam etc.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Snowball
    &lt;ul&gt;
      &lt;li&gt;Snowball - Pure storage 50TB to 80TB&lt;/li&gt;
      &lt;li&gt;Snowball Edge - Both Compute and Storage Capabilities&lt;/li&gt;
      &lt;li&gt;Snowmobile - 100PB worth of storage&lt;/li&gt;
      &lt;li&gt;Understand what snowball is&lt;/li&gt;
      &lt;li&gt;Understand what Import/Export is&lt;/li&gt;
      &lt;li&gt;Snowball can import / Export to S3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 Transfer Acceleration
    &lt;ul&gt;
      &lt;li&gt;You can speed up S3 transfer speeds using CloudFront Edge locations, so you can upload files to s3 faster. Extra costs will apply. Greatest Impact on people who are further away geographically.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 static websites.
    &lt;ul&gt;
      &lt;li&gt;You can S3 to host static websites.&lt;/li&gt;
      &lt;li&gt;Serverless&lt;/li&gt;
      &lt;li&gt;Very Cheap, scales automatically.&lt;/li&gt;
      &lt;li&gt;Static only, cannot host dynamic sites.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CORS
    &lt;ul&gt;
      &lt;li&gt;Cross origin resource sharing&lt;/li&gt;
      &lt;li&gt;Needed to enable cors on the resources bucket and state the URL for the origin that will be calling the bucket.&lt;/li&gt;
      &lt;li&gt;Remeber to use the S3 website url and not the bucket URL, website url has the word website on it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When you do a PUT object on S3 you will get a HTTP 200 success code.&lt;/li&gt;
  &lt;li&gt;You can load files much faster by enabling multipart upload.&lt;/li&gt;
  &lt;li&gt;Read the S3 FAQ!&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="s3" /><category term="summary" /><summary type="html">S3 - Exam Tips S3 is an Object based storage, its a place where you upload and download files. You cannot run an os, or run a database off of S3. Files can be of size 0 bytes all the way to 5TB. There is unlimited storage. Files are stored in Buckets. S3 is a universal namespace, all bucket names shall be unique globally. S3 bucket names are like https://s3-[region].amazonaws.com/[bucketname] Read after Write Consistency for PUTS of new objects. Eventual Consistencry for Overwrite PUTS and DELETES. ( takes some time to propergate ) S3 Storage Classes / Tiers Standard S3 reliability of 99.999999999% availability of 99.99% Immedietly available and frequently accessed S3 Infrequent Access ( IA ) availablity of 99.99% reliability of 99.999999999% Immedietly available infrequently accessed. Charged for retreval S3 Reduced Redundency reliability and availability of 99.99% Used for reproducable content ( such as thumbnails etc ). Glacier Archived data, where you have to wait 3 - 5 hour waiting period. Core fundementals of S3 Key ( name ) Value ( data ) Version ID Metadata ( data about data ) Access control lists Object based stroage only ( flat files ) S3 versioning Stores all versions of an object ( including all writes and even if you delete an object ) Great backup tool You pay for storage for all the versions of the object. Once enabled versioning cannot be disabled, it can only be suspended. Integrates with lifecycle rules. Versionings MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. Cross region replication requires versioning enabled on both the source bucket and destination bucket. Lifecycle Management Can be used in conjunction with versioning. Can be applied to current versions and previous versions. Following actions can now be done. Transition to infrequenct access after a minimum of 30 days since creation date ( files need to be minimum 128kb ) Archived to glacier storage class ( 30 days after IA, if relevant ). Permenantly Delete Objects CloudFront Edge Locations - This is the location where content will be cached. Origin - This is the origin of all the files that the CDN will distribute. This can be either an S3 bucket, an EC2 instance, an Elastic Load balancer or Route53. Distribution - This is the name given to the CDN with a collections of Edge Locations. Web Distribution - Typically used for websites RTMP - Used for Media Streaming Edge locations are not just readonly, you can write to them as well. ( Upload files to S3 ). Object are cached for the life of the TTL ( time to live ) ( default ttl 24hours ) You can clear cached objects, but you will be charged. Securing your buckets. By default, all newly created buckets are PRIVATE. You can setup access control to your buckets using; Bucket policies. Access Control Lists. S3 buckets can be configured to create access logs which logs all the access requests made to the bucke. These logs can be stored in another bucket. Encryption In-transit Encryption SSL/TLS (HTTPS) In rest Server Side Encryption S3 Managed Keys SSE-S3 AWS Managed Keys SSE-KMS Allows you to monitor who decrypted which file when. (Audit trail) Customer provided Keys SSE-C Client Side Encryption Client uploads encrypted files.</summary></entry><entry><title type="html">Storage Gateway</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/Storage-Gateway.html" rel="alternate" type="text/html" title="Storage Gateway" /><published>2018-06-13T00:00:00+10:00</published><updated>2018-06-13T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/Storage-Gateway</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/Storage-Gateway.html">&lt;p&gt;AWS Storage Gateway, is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization’s on premises IT environment and AWS’s storage infrastructure. The service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.&lt;/p&gt;

&lt;p&gt;AWS Storage Gateway Software Appliance is available for download as a virtual machine (VM) image that you install on a host in your data center. Storage Gateway support either VMware ESXi or Microsoft Hyper-V. Once youve installed your gateway and associated it with your AWS account though the activation process, you use the AWS Management Console to create the storage gateway option that is right for you.&lt;/p&gt;

&lt;h3 id=&quot;4-types-of-storage-gateways&quot;&gt;4 types of storage gateways&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;File Gateway (NFS)&lt;/li&gt;
  &lt;li&gt;Allows you store flat files in S3 ( word documents, pictures, pdf’s, etc )&lt;/li&gt;
  &lt;li&gt;Volume Gateways ( iSCSI ) This for block based storage, maybe a volume attached to a VM.
    &lt;ul&gt;
      &lt;li&gt;Stored Volume ( Gateway Stored Volumes )&lt;/li&gt;
      &lt;li&gt;Cached Volume ( Gateway Cached Volumes )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tape Gateway ( VTL ) Backup and archiving solution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;file-gateway&quot;&gt;File Gateway&lt;/h3&gt;
&lt;p&gt;Files are stored as objects in your S3 buckets, accessed through a Network File System (NFS) mount point. Ownership, permissions, and timestamps are durably stored in S3 in the user-metadata of the object associated with the file. Once objects are transferred to S3, they can be managed as native S3 objects, and bucket policies such as versioning, lifecycle management, and cross-region replication apply directly to objects stored in your bucket.&lt;/p&gt;

&lt;h3 id=&quot;volume-gateway&quot;&gt;Volume Gateway&lt;/h3&gt;
&lt;p&gt;The volume interface presents your applications with disks volumes using iSCSI block protocol. Data written to these volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots. Snapshots are incremental backups that capture only changed blocks. All snapshots storage is also compressed to minimize your storage charges.&lt;/p&gt;

&lt;h5 id=&quot;stored-volumes&quot;&gt;Stored Volumes&lt;/h5&gt;
&lt;p&gt;Stored volumes let you store your primary data locally, while asynchronously backing up that data to AWS. Stored volumes provide your on-premises applications with low-latency access to their entire datasets, while providing durable, off-site backups. You can create storage volumes and mount them as iSCSI devices from your on-premises application servers. Data written to your stored volumes is asynchronously backed upto Amazon Simple Storage Service ( Amazon S3 ) in the form of Amazon Elastic Block Store ( Amazon EBS ) snapshots. 1GB - 16TB in size for Stored Volumes.&lt;/p&gt;

&lt;h5 id=&quot;cached-volumes&quot;&gt;Cached Volumes&lt;/h5&gt;
&lt;p&gt;Cached volumes lets you use Amazon Simple Storage Service ( Amazon S3 ) as your primary data storage while retaining frequently accessed data locally in your storage gateway. Cached volumes minimize the need to scale your on-premises storage infrastructure, while still providing your applications with low-latency access to their frequently accessed data. You can create storage volumes upto 32 TB in size and attach to them as iSCSI devices from your on-premises application servers. Your gateway stores data that you write to these volumes in Amazon S3 and retains recently read data in your on-premises storage gateway’s cache and upload buffer storage. 1GB - 32 TB in size for Cached Volumes.&lt;/p&gt;

&lt;h5 id=&quot;taped-gateway&quot;&gt;Taped Gateway&lt;/h5&gt;
&lt;p&gt;Tape Gateway offers a durable, cost-effective solution to archive your data in the AWS Cloud. The VTL interface it provides lets you leverage your existing tape-based backup application infrastructure to store data on virtual tape cartridges that you create on your tape gateway. Each tape gateway is preconfigured with a media changer and tape drives, which are available to your existing client backup applications as iSCSI devies. You add tape cartridges as you to archive your data. Supported by NetBackup, Backup Exec, Veam etc.&lt;/p&gt;

&lt;h3 id=&quot;exam-tips&quot;&gt;Exam Tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;File Gateway - For flat files, stored directly on S3 - Not os’s&lt;/li&gt;
  &lt;li&gt;Volumes Gateways (block based storage)
    &lt;ul&gt;
      &lt;li&gt;Stored Volumes - Entire Dataset is stored on site is asynchronously backed up to S3.&lt;/li&gt;
      &lt;li&gt;Cached Volumes - Entire Dataset is stored on S3 and most frequently accessed data is kept on site.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gateway Virtual Tape Library ( VTL )
    &lt;ul&gt;
      &lt;li&gt;Used for backup and uses popular backup applications like NetBackup, Backup Exec, Veam etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="storagegateway" /><category term="secure" /><summary type="html">AWS Storage Gateway, is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization’s on premises IT environment and AWS’s storage infrastructure. The service enables you to securely store data to the AWS cloud for scalable and cost-effective storage.</summary></entry><entry><title type="html">S3 Security and Encryption</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Security-and-Encryption.html" rel="alternate" type="text/html" title="S3 Security and Encryption" /><published>2018-06-13T00:00:00+10:00</published><updated>2018-06-13T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Security-and-Encryption</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Security-and-Encryption.html">&lt;h3 id=&quot;security&quot;&gt;Security&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;By default, all newly created buckets are &lt;strong&gt;PRIVATE&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;You can setup access control to your buckets using;
    &lt;ul&gt;
      &lt;li&gt;Bucket Policies - Permissions are applied to the entier bucket.&lt;/li&gt;
      &lt;li&gt;Access Control List - You can apply different permissions to different objects in the buckets.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 Buckets can be configured to create access logs which log all the requests made to the S3 bucket. This can be done to another bucket.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;encryption&quot;&gt;Encryption&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In transit ( When sending object to and from your PC )
    &lt;ul&gt;
      &lt;li&gt;SSL/TLS Encryption ( HTTPS )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;At Rest ( When the objects are either in your PC or in the Server )
    &lt;ul&gt;
      &lt;li&gt;Server Side Encryption
        &lt;ul&gt;
          &lt;li&gt;S3 Managed Keys - SSE-S3&lt;/li&gt;
          &lt;li&gt;AWS Key Management Service ( AWS KMS ) - Managed Keys - SSE-KMS
            &lt;ul&gt;
              &lt;li&gt;These keys provide an audit trail, lets you know who has been using the keys.&lt;/li&gt;
              &lt;li&gt;Who has been decrypting what and when was it done.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Server Side Encryption with Customer Provided Keys - SSE-C&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Client Side Encryption ( You encrypt the files and then you upload them to S3 )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="s3" /><category term="security" /><category term="encryption" /><category term="secure" /><category term="administration" /><category term="storage" /><summary type="html">Security By default, all newly created buckets are PRIVATE. You can setup access control to your buckets using; Bucket Policies - Permissions are applied to the entier bucket. Access Control List - You can apply different permissions to different objects in the buckets. S3 Buckets can be configured to create access logs which log all the requests made to the S3 bucket. This can be done to another bucket.</summary></entry><entry><title type="html">S3 Lifecycle Management</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Lifecycle-Management.html" rel="alternate" type="text/html" title="S3 Lifecycle Management" /><published>2018-06-13T00:00:00+10:00</published><updated>2018-06-13T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Lifecycle-Management</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Lifecycle-Management.html">&lt;ul&gt;
  &lt;li&gt;Can be used in conjunction with versioning&lt;/li&gt;
  &lt;li&gt;Can be applied to current versions and previous versions.&lt;/li&gt;
  &lt;li&gt;Following actions can now be done:
    &lt;ul&gt;
      &lt;li&gt;Transition to the standard infrequently accessed storage class
        &lt;ul&gt;
          &lt;li&gt;File sizes have to be bigger than 128kb and a minimum 30 days after creation.&lt;/li&gt;
          &lt;li&gt;Archive it to glacier storage class ( 30 days after IA, if relevant )&lt;/li&gt;
          &lt;li&gt;Permanently Delete&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="s3" /><category term="lifecycle" /><category term="management" /><summary type="html">Can be used in conjunction with versioning Can be applied to current versions and previous versions. Following actions can now be done: Transition to the standard infrequently accessed storage class File sizes have to be bigger than 128kb and a minimum 30 days after creation. Archive it to glacier storage class ( 30 days after IA, if relevant ) Permanently Delete</summary></entry><entry><title type="html">S3 Cross Region Replication</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Cross-Region-Replication.html" rel="alternate" type="text/html" title="S3 Cross Region Replication" /><published>2018-06-13T00:00:00+10:00</published><updated>2018-06-13T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Cross-Region-Replication</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/S3-Cross-Region-Replication.html">&lt;h3 id=&quot;instructions&quot;&gt;Instructions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Create 2 buckets one in london, and sydney&lt;/li&gt;
  &lt;li&gt;Enable Cross Region Replication on the london bucket
    &lt;ul&gt;
      &lt;li&gt;In order for Cross Region Replication to work, you need both buckets with versioning enabled.&lt;/li&gt;
      &lt;li&gt;You can enable CRR on just a prefix on a bucket.&lt;/li&gt;
      &lt;li&gt;Set the bucket storage class from standard to IA.&lt;/li&gt;
      &lt;li&gt;Hit save!&lt;/li&gt;
      &lt;li&gt;Existing objects will not be replicated into both the buckets, Only new objects get replicated into the other buckets&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If you want to copy your current items in bucket one to bucket two.
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recursive&lt;/span&gt; s3://[bucket_1] s3://[bucket_2]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;Items that have been copied over, by default has permission set to private.&lt;/li&gt;
      &lt;li&gt;If you delete an item from bucket 1, it sets a delete marker on bucket 1 and does the same on bucket 2.&lt;/li&gt;
      &lt;li&gt;If you delete the delete marker on bucket 1 it does not delete on bucket 2.&lt;/li&gt;
      &lt;li&gt;If you revert to an older version in bucket 1, you must do so in bucket 2, because the changes are not refelcted automatically in bucket 2.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;exam-tips&quot;&gt;Exam Tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Versioning Must be enabled on both the source and destination buckets&lt;/li&gt;
  &lt;li&gt;Region must be unique.&lt;/li&gt;
  &lt;li&gt;Files in an existing must be replicated across regions manually, otherwise it does not get replicated automatically.&lt;/li&gt;
  &lt;li&gt;You cannot replicate to multiple bucket or daisy chain it at this time.&lt;/li&gt;
  &lt;li&gt;delete markers are replicated.&lt;/li&gt;
  &lt;li&gt;deleting individual version or delete markers will not be replicated.&lt;/li&gt;
  &lt;li&gt;Understanding what cross region replication is at a high level.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="crossregionreplication" /><category term="s3" /><category term="storage" /><summary type="html">Instructions</summary></entry><entry><title type="html">CloudFront</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/CloudFront.html" rel="alternate" type="text/html" title="CloudFront" /><published>2018-06-13T00:00:00+10:00</published><updated>2018-06-13T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/CloudFront</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/13/CloudFront.html">&lt;p&gt;A content delivery network (CDN) is a system of distributed servers (network) that delivers webpages and other web content to a user based on the geographic locations of the user, the origin of the webpage and a content delivery server.&lt;/p&gt;

&lt;p&gt;Amazon cloudfront can be used to deliver your entire website, including dynamic, static, streaming, and interactive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance.&lt;/p&gt;

&lt;p&gt;Amazon CloudFront is optimized to work with other Amazon web services, like amazon simple storage service, amazon elastic cloud compute, amazon elastic load balancing, and amazon route 53. Amazon CloudFront also works seamlessly  with any Non-Aws origin server, which stores the original, definitive versions of your files.&lt;/p&gt;

&lt;h3 id=&quot;key-terminology&quot;&gt;Key terminology&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Edge Location&lt;/strong&gt; - This is the location where content will be cached. This is seperate to an AWS Region/AZ.
    &lt;ul&gt;
      &lt;li&gt;Edge Locations are not &lt;strong&gt;READ-ONLY&lt;/strong&gt;. You can &lt;strong&gt;WRITE&lt;/strong&gt; to them too. ( ie put an object on to them ). This will be replicated back to the origin server.&lt;/li&gt;
      &lt;li&gt;Objects are cached for the life of the TTL ( Time To Live )&lt;/li&gt;
      &lt;li&gt;You can clear cached objects, but you will be charged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Origin&lt;/strong&gt; - This is the origin of all the files that the CDN will distribute. This can be either an S3 bucket, an EC2 Instance, an Elastic Load Balancer or Route53.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distribution&lt;/strong&gt; - This is the name given to the CDN which consists of a collection of Edge Locations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Web Distribution&lt;/strong&gt; - Typically used for Websites.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RTMP&lt;/strong&gt; - Used for Media Streaming.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lab&quot;&gt;LAB&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Create a bucket in s3 to be used with CloudFront.&lt;/li&gt;
  &lt;li&gt;Upload an image to that bucket.&lt;/li&gt;
  &lt;li&gt;Goto CloudFront and click on create distribution network.&lt;/li&gt;
  &lt;li&gt;Click on get start under Web.&lt;/li&gt;
  &lt;li&gt;Select the s3 bucket on the origin domain name. (dropdown menu)&lt;/li&gt;
  &lt;li&gt;Select Yes for restricting bucket access.&lt;/li&gt;
  &lt;li&gt;Create a new identity for bucket access.&lt;/li&gt;
  &lt;li&gt;Update bucket policy to grant read permissions.&lt;/li&gt;
  &lt;li&gt;Under default cache behaviour Viewer protocol policy Redirect HTTP to HTTPS&lt;/li&gt;
  &lt;li&gt;Restrict Viewer Access ( Use signed urls or signed cookies ) if you want to have restrictions.&lt;/li&gt;
  &lt;li&gt;Hit save and it should be deployed in around 15 minutes.&lt;/li&gt;
  &lt;li&gt;You can also enable geo restrictions
    &lt;ul&gt;
      &lt;li&gt;either black countries or whitelist countries but not both.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If you want to remove an object from the cache immedietely ( Costs MONEY ), you can do so by creating an Invalidation.&lt;/li&gt;
  &lt;li&gt;After youre done with setting up, make sure you delete the distribution network because this does cost you money.
    &lt;ul&gt;
      &lt;li&gt;First select the distribution network, and click on disable (15 - 30) mins later you will be able to click on delete.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="cloudfront" /><category term="cdn" /><category term="s3" /><summary type="html">A content delivery network (CDN) is a system of distributed servers (network) that delivers webpages and other web content to a user based on the geographic locations of the user, the origin of the webpage and a content delivery server.</summary></entry><entry><title type="html">S3 Enable Versioning</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/12/S3-Enable-Versioning.html" rel="alternate" type="text/html" title="S3 Enable Versioning" /><published>2018-06-12T00:00:00+10:00</published><updated>2018-06-12T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/12/S3-Enable-Versioning</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/12/S3-Enable-Versioning.html">&lt;h3 id=&quot;instructions&quot;&gt;Instructions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Loggin to the aws console and head over to aws s3&lt;/li&gt;
  &lt;li&gt;Create a new bucket, click next instead of create.&lt;/li&gt;
  &lt;li&gt;Enable versioning and click next until bucket is created.&lt;/li&gt;
  &lt;li&gt;Once versioning has been enabled for a bucket.
    &lt;ul&gt;
      &lt;li&gt;You cannot remove versioning from the bucket.&lt;/li&gt;
      &lt;li&gt;You can stop the bucket from creating more versions.&lt;/li&gt;
      &lt;li&gt;If you want to completely remove versioning from the bucket, create a new bucket, transfer everything to that bucket.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If you deleted an object, you can restore that object as well. You will be able to restore it to any point in time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tips&quot;&gt;Tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Stores all versions of an object (including all writes and even if you object)&lt;/li&gt;
  &lt;li&gt;Great Backup tool&lt;/li&gt;
  &lt;li&gt;Once enabled, Versioning cannot be disabled, only suspended.&lt;/li&gt;
  &lt;li&gt;Integrated with Life cycle rules.&lt;/li&gt;
  &lt;li&gt;Versioning’s MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. ( bucket level and object level )&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="s3" /><category term="versioning" /><summary type="html">Instructions Loggin to the aws console and head over to aws s3 Create a new bucket, click next instead of create. Enable versioning and click next until bucket is created. Once versioning has been enabled for a bucket. You cannot remove versioning from the bucket. You can stop the bucket from creating more versions. If you want to completely remove versioning from the bucket, create a new bucket, transfer everything to that bucket. If you deleted an object, you can restore that object as well. You will be able to restore it to any point in time.</summary></entry><entry><title type="html">Serverless Website</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/11/Serverless-Website.html" rel="alternate" type="text/html" title="Serverless Website" /><published>2018-06-11T00:00:00+10:00</published><updated>2018-06-11T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/11/Serverless-Website</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/11/Serverless-Website.html">&lt;p&gt;This will be example of the simplest serverless website possible.&lt;/p&gt;
&lt;h3 id=&quot;instructions&quot;&gt;Instructions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;First we will need to create a lambda function
    &lt;ul&gt;
      &lt;li&gt;Select python 3.6 for the language&lt;/li&gt;
      &lt;li&gt;add the following code to your lambda&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lambda_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;In lambda_handler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;statusCode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;headers&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;Access-Control-Allow-Origin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;body&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;This response is from a lambda function running in ohio!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Now set the trigger for your lambda function as API Gateway.
    &lt;ul&gt;
      &lt;li&gt;Give it any name you want.&lt;/li&gt;
      &lt;li&gt;Save your lambda settings&lt;/li&gt;
      &lt;li&gt;Go to API Gateway and make sure its a get request.&lt;/li&gt;
      &lt;li&gt;Go to stages and get the GET method URL.&lt;/li&gt;
      &lt;li&gt;See if the URL gives back the response you expect.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Create a bucket in S3 with static website hosting&lt;/li&gt;
  &lt;li&gt;add the following index.html&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;xhttp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;XMLHttpRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;xhttp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;onreadystatechange&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;readyState&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;status&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementById&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;demo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;innerHTML&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;responseText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;xhttp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;GET&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;API GATEWAY URL&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;xhttp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;align=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;center&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&amp;gt;&lt;/span&gt;Hello people&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;onclick=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;request()&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Click me!&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;demo&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;If you have added the API Gateway url in the code above, It should work accordingly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And that is how you build a serverless website with S3, Lambda and API Gateway!&lt;/p&gt;</content><author><name>Eshan Shafeeq</name></author><category term="compute" /><category term="storage" /><category term="apigateway" /><category term="s3" /><category term="lambda" /><category term="serverless" /><summary type="html">This will be example of the simplest serverless website possible. Instructions First we will need to create a lambda function Select python 3.6 for the language add the following code to your lambda</summary></entry></feed>