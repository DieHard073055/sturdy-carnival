<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.2">Jekyll</generator><link href="https://diehard073055.github.io//sturdy-carnival/feed.xml" rel="self" type="application/atom+xml" /><link href="https://diehard073055.github.io//sturdy-carnival/" rel="alternate" type="text/html" /><updated>2018-06-22T01:19:51+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/</id><title type="html">AWS Developer Associate Notes</title><subtitle>Blog to keep track of what i have been studying for AWS Developer Associate Course. Also will be referrence to study before the exam.</subtitle><entry><title type="html">DynamoDB Summary</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/22/DynamoDB-Summary.html" rel="alternate" type="text/html" title="DynamoDB Summary" /><published>2018-06-22T00:00:00+10:00</published><updated>2018-06-22T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/22/DynamoDB-Summary</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/22/DynamoDB-Summary.html">&lt;h3 id=&quot;what-is-dynamodb&quot;&gt;What is dynamoDB&lt;/h3&gt;
&lt;p&gt;Fast and flexible NOSQL database service for all application that need consistent, single digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT and many other applications.&lt;/p&gt;

&lt;h3 id=&quot;quick-facts&quot;&gt;Quick Facts&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Stored on SSD Storage.&lt;/li&gt;
  &lt;li&gt;Spread Across 3 geographically distinct data centres.&lt;/li&gt;
  &lt;li&gt;Eventaul Consistent Reads (Default)
    &lt;ul&gt;
      &lt;li&gt;Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best read performance).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Strong Consistent Reads
    &lt;ul&gt;
      &lt;li&gt;A strongly consistent read returns a result that reflects all writes that received a successful response prior to the read.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;basics&quot;&gt;Basics&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* Tables
* Items ( Data in table )
* Attributes ( Column names in a table )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;primary-keys&quot;&gt;Primary Keys&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Two types of primary keys available&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Single Attribute (Unique ID)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Partition Key (Hash key) composed of one attribute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Composite (Think unique ID and a date range)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Partition Key &amp;amp; Sort Key (Hash &amp;amp; Range) composed of two attributes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partition Key&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;DynamoDB uses the partition key’s value as input to an internal hash function. The output from the hash function determines the partition ( this is simply the physical location in which the data is stored).&lt;/li&gt;
      &lt;li&gt;No two items in a table can have the same partition key value!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partition Key and Sort Key&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition ( this is simply the physical location in which the data is stored).&lt;/li&gt;
      &lt;li&gt;Two items can have the same parition key, but they &lt;strong&gt;must have a different sort key.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;All items with the same partition key will be stored together but ordered using the sort key value.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;indexes&quot;&gt;Indexes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Local Secondary Index&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Has the SAME parition key, different sort key.&lt;/li&gt;
  &lt;li&gt;Can ONLY be created when creating a table. They cannot be removed or modified later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Global Secondary Index&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Has a DIFFERENT partition key and DIFFERENT sort key.&lt;/li&gt;
  &lt;li&gt;Can be created at table creation or added LATER.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamodb-streams&quot;&gt;DynamoDB Streams&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Used to capture any kind of modification to the DynamoDB tables.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If a new item is added to the table, the stream captures an image of the entire item, including all of its attributes.&lt;/li&gt;
  &lt;li&gt;If an item is updated, the stream captures the “before” and “after” image of any attributes that were modified in the item.&lt;/li&gt;
  &lt;li&gt;If an item is deleted from the table, the stream captures an image of the entire item before it was deleted.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;query--scans&quot;&gt;Query &amp;amp; Scans&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A Query operation finds items in a table using only primary key attribute values. You must provide a partition key attribute name and a distint value to search for.&lt;/li&gt;
  &lt;li&gt;A Scan operation examines every item in the table. By default,  a Scan returns all of the data attributes for every item; however you can use the &lt;strong&gt;ProjectionExpression&lt;/strong&gt; parameter so that the Scan only returns some of the attributes, rather than all of them.&lt;/li&gt;
  &lt;li&gt;Try to use a Query operation rather than a Scan operation as it is more efficient.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamodb-provisioned-throughput-calculation&quot;&gt;DynamoDB Provisioned Throughput Calculation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Read 13 items of 17 KB per second (using eventual consistency)
    &lt;ol&gt;
      &lt;li&gt;17KB to the nearest 4KB = 20&lt;/li&gt;
      &lt;li&gt;20KB / 4KB = 5 read units per item&lt;/li&gt;
      &lt;li&gt;5 x 13 read items per second = 65&lt;/li&gt;
      &lt;li&gt;65 / 2 for eventual consistency = 32.5 (33)&lt;/li&gt;
      &lt;li&gt;33 units of read throughput&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Write 94 items of 3KB per second
    &lt;ol&gt;
      &lt;li&gt;3KB to the nearest 1KB = 3&lt;/li&gt;
      &lt;li&gt;3KB/1KB = 3 write units per item&lt;/li&gt;
      &lt;li&gt;3 x 94 write items per second = 282&lt;/li&gt;
      &lt;li&gt;282 / 1&lt;/li&gt;
      &lt;li&gt;282 units of write throughput capacity&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Error code
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;400 HTTP ProvisionedThroughputExceededException&lt;/strong&gt; - You have exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;steps-taken-to-authenticate-with-web-identity-provider&quot;&gt;Steps taken to authenticate with Web Identity Provider&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;User authenticates with Web identity Provider&lt;/li&gt;
  &lt;li&gt;They are passed a Token by their ID Provider&lt;/li&gt;
  &lt;li&gt;Your code calls &lt;strong&gt;AssumeRoleWithWebIdentity&lt;/strong&gt; API and provides the providers token and specifies the ARN for the IAM Role.&lt;/li&gt;
  &lt;li&gt;App can now access DynamoDB from between 15 minutes to 1 hour (default 1 hour).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conditional-write&quot;&gt;Conditional Write&lt;/h3&gt;
&lt;p&gt;If a price equals &lt;strong&gt;X&lt;/strong&gt; then update the price to &lt;strong&gt;Y&lt;/strong&gt;. If the price is not equal to &lt;strong&gt;X&lt;/strong&gt; operation fails. This is an &lt;strong&gt;Idempotent&lt;/strong&gt; operation.&lt;/p&gt;

&lt;h3 id=&quot;atomic-counters&quot;&gt;Atomic Counters&lt;/h3&gt;
&lt;p&gt;Increment or Decrement a value on an existing attirbute, regardless of its value. This is not an Idempotent operation. May over-count or under-count.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use conditional writes for critical stuff, atomic counters for non critical stuff.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-operations&quot;&gt;Batch Operations&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;BatchGetItem&lt;/strong&gt; can contain upto 1MB of data and upto 100 items&lt;/li&gt;
  &lt;li&gt;Can retrieve items from multiple tables.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;READ THE FAQ OF DYNAMODB&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name>Eshan Shafeeq</name></author><category term="summary" /><category term="dynamodb" /><category term="database" /><summary type="html">What is dynamoDB Fast and flexible NOSQL database service for all application that need consistent, single digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT and many other applications.</summary></entry><entry><title type="html">DynamoDB - Other Key Facts</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/22/DynamoDB-Other-Key-Facts.html" rel="alternate" type="text/html" title="DynamoDB - Other Key Facts" /><published>2018-06-22T00:00:00+10:00</published><updated>2018-06-22T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/22/DynamoDB---Other-Key-Facts</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/22/DynamoDB-Other-Key-Facts.html">&lt;h4 id=&quot;conditional-writes&quot;&gt;Conditional Writes&lt;/h4&gt;
&lt;p&gt;Example Scenario&lt;/p&gt;

&lt;p&gt;if two clients were to update a piece of data at the same time, the updates may happen at 2 different facilities, and may create unexpected outcome. In order to prevent this use, conditional writes. Lets say you want to update a price of an item, but you only want to update if its $6. So the first user do UpdateItem if Price is $6 to $15. The Price will change, the second user will also try and do a UpdateItem if Price is $6, but this time the price is 15 and the operation will fail.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;These conditional writes are idempotent. This means that you can send the same conditional write request multiple times, but will have no further effect on the item after the first successful update.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DynamoDB support atomic counters, where you use the UpdateItem operation to increment or decrement the value of an existing attribute without interfering with other write requests. (All write requests are applied in the order in which they were recieved). For example, a web application might want to maintain a counter per visitor to their site. In this case, the application would need to increment this counter regardless of its current value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Atomic counter updates are not idempotent. This means the counter will increment each time you call UpdateItem. If you suspect that a previous request was unsuccessful, your application could retry the UpdateItem operation; however this would risk updating the counter twice. This might be acceptable for a web site counter, because you can tolerate with slightly over-or under-counting the visitors. However in a banking application, it would be safer to use a conditional update rather than an atomic counter.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;In the exam you will be asked if you should use the atomic counter or the conditional update, it depends if the data is critical, example voting or banking you would use conditional writes. Where as for website visitor counters you would use atomic counters.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your application needs to read multiple items, you can use the BatchGetItem API. A single &lt;strong&gt;&lt;em&gt;BatchGetItem&lt;/em&gt;&lt;/strong&gt; request can retrieve up to 1 MB of data, which can contain as many as 100 items. In addition, a single BatchGetItem request can retrieve items from multiple tables.&lt;/p&gt;</content><author><name>Eshan Shafeeq</name></author><category term="dynamodb" /><category term="database" /><category term="keyfacts" /><summary type="html">Conditional Writes Example Scenario</summary></entry><entry><title type="html">Web Identity Providers</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/21/Web-Identity-Providers.html" rel="alternate" type="text/html" title="Web Identity Providers" /><published>2018-06-21T00:00:00+10:00</published><updated>2018-06-21T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/21/Web-Identity-Providers</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/21/Web-Identity-Providers.html">&lt;h3 id=&quot;web-identity-providers&quot;&gt;Web Identity Providers&lt;/h3&gt;

&lt;p&gt;You can authenticate users using &lt;strong&gt;Web Identity Providers&lt;/strong&gt; ( such as Facebook, Google, Amazon, or any other Open-ID Connect-compatible Identity provider). This is done using AssumeRoleWithWebIdentity API.&lt;/p&gt;

&lt;p&gt;You will need to create a role first&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;User Authenticates With WebIDProvider&lt;/li&gt;
  &lt;li&gt;WebIDProvider gives you a Web Identity Token.&lt;/li&gt;
  &lt;li&gt;Do the AssumeRoleWithWebIdentity Request to AWS SecurityTokenService (STS)&lt;/li&gt;
  &lt;li&gt;AWS (STS) gives back Temporary Security Credentials ( Defaults 1 hour )
    &lt;ul&gt;
      &lt;li&gt;Access Key ID, Secret Access Key, Session Token.&lt;/li&gt;
      &lt;li&gt;Expiration ( Time limit )&lt;/li&gt;
      &lt;li&gt;AssumeRoleID&lt;/li&gt;
      &lt;li&gt;SubjectFromWebIdentityToken ( The unique ID that appears in an IAM policy variable for this particular identity provider )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using these credentials, you can access dynamodb.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;steps-taken-to-authenticate&quot;&gt;Steps taken to authenticate&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;User Authenticates with the ID provider&lt;/li&gt;
  &lt;li&gt;They are passed a Token by their ID provider.&lt;/li&gt;
  &lt;li&gt;Your code calls &lt;strong&gt;AssumeRoleWithWebIdentity&lt;/strong&gt; API and provides the providers token and specifies the ARN for the IAM Role.&lt;/li&gt;
  &lt;li&gt;App can now access DynamoDB from between 15 minutes to 1 hour (default is 1 hour)&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Eshan Shafeeq</name></author><category term="security" /><category term="database" /><category term="social" /><category term="openid" /><category term="assumerolewithwebidentity" /><category term="dynamodb" /><summary type="html">Web Identity Providers</summary></entry><entry><title type="html">DynamoDB Provisioned Throughput Calculations</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/21/DynamoDB-Provisioned-Throughput-Calculations.html" rel="alternate" type="text/html" title="DynamoDB Provisioned Throughput Calculations" /><published>2018-06-21T00:00:00+10:00</published><updated>2018-06-21T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/21/DynamoDB-Provisioned-Throughput-Calculations</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/21/DynamoDB-Provisioned-Throughput-Calculations.html">&lt;h3 id=&quot;provisioned-throughput&quot;&gt;Provisioned Throughput&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Unit of Read provisioned throughput
    &lt;ul&gt;
      &lt;li&gt;All reads are rounded up to increments of 4KB&lt;/li&gt;
      &lt;li&gt;Eventually Consistent Reads (default) consist of 2 reads per second.&lt;/li&gt;
      &lt;li&gt;Strongly Consistent Reads consist of 1 read per second.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unit of Write provisioned throughput.
    &lt;ul&gt;
      &lt;li&gt;All writes are 1 KB&lt;/li&gt;
      &lt;li&gt;All writes consist of 1 write per second&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-magic-formula&quot;&gt;The magic formula&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Question&lt;/strong&gt; - you have an application that requires to read 10 items of 1KB per second using eventual consistency. What should you set the read throughput to?&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;(Size of Read rounded to nearest 4KB chunk / 4KB) x no of items = read throughput&lt;/strong&gt;
&lt;strong&gt;Divide by 2 if eventually consistent.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* First we calculate how many read units per item we need.

* 1KB rounded to the nearest 4KB increment = 4KB
* 4KB / 4KB = 1 read unit per item.

* 1 x 10 read items = 10
* Using eventual consistency we get 10 / 2 = 5
* 5 units of read throughput
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;You have an application that requires to read 10 items of 6 KB per second using eventual consistency. What should you set the read throughput to?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6 KB rounded to the nearest 4KB increment = 8KB&lt;/li&gt;
      &lt;li&gt;8KB / 4KB = 2 read unit per item&lt;/li&gt;
      &lt;li&gt;2 x 10 read items = 20&lt;/li&gt;
      &lt;li&gt;Using eventual consistency we get 20 / 2 = 10&lt;/li&gt;
      &lt;li&gt;10 units of read throughput&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;You have an application that requires to read 5 items of 10KB per second using eventual consistency. What should you set the read throughput to?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;10KB rounded to the nearest 4KB increment = 12KB&lt;/li&gt;
      &lt;li&gt;12KB / 4KB = 3 read units per item&lt;/li&gt;
      &lt;li&gt;3 x 5 read items = 15&lt;/li&gt;
      &lt;li&gt;using eventual consistency we get 15 / 2 = 7.5 (8) &amp;lt;- since read throughput can only be integer.&lt;/li&gt;
      &lt;li&gt;8 units of read throughput.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;You have an application that requires to read 5 items of 10KB per second using **&lt;em&gt;strong consistency&lt;/em&gt;&lt;/strong&gt;. What should you set the read throughput to?**
    &lt;ul&gt;
      &lt;li&gt;10KB rounded to the nearest 4KB increment = 12KB&lt;/li&gt;
      &lt;li&gt;12KB / 4KB = 3 read units per item&lt;/li&gt;
      &lt;li&gt;3 x 5 read items = 15&lt;/li&gt;
      &lt;li&gt;Since we are using strong consistency 15 x 1&lt;/li&gt;
      &lt;li&gt;15 units of read throughput&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;You have an application that requires to write 5 items, with each item being 10KB in size. What should the write throughput be set to?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;10 KB rounded to the nearest 1KB = 10KB&lt;/li&gt;
      &lt;li&gt;10 KB / 1 KB = 10 write units per item&lt;/li&gt;
      &lt;li&gt;10 x 5 write items = 50&lt;/li&gt;
      &lt;li&gt;50 units of write throughput&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;You have an application that requires to write 12 items, with each item being 100KB in size. What should the write throughput be set to?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;100 KB rounded to the nearest 1KB = 100KB&lt;/li&gt;
      &lt;li&gt;100 KB / 1 KB = 100 write units per item&lt;/li&gt;
      &lt;li&gt;100 x 12 write items = 1200&lt;/li&gt;
      &lt;li&gt;1200 units of write throughput&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;error-codes&quot;&gt;Error Codes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;400 HTTP Status Code &lt;strong&gt;ProvisionedThroughputExceededException&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;You have exceeded the maximum allowed provisioned throughput for a table or for one or global secondary indexes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="dynamodb" /><category term="database" /><category term="cost" /><category term="throughput" /><category term="calculations" /><summary type="html">Provisioned Throughput Unit of Read provisioned throughput All reads are rounded up to increments of 4KB Eventually Consistent Reads (default) consist of 2 reads per second. Strongly Consistent Reads consist of 1 read per second. Unit of Write provisioned throughput. All writes are 1 KB All writes consist of 1 write per second</summary></entry><entry><title type="html">Scan vs Query API Calls</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/15/Scan-vs-Query-API-Calls.html" rel="alternate" type="text/html" title="Scan vs Query API Calls" /><published>2018-06-15T00:00:00+10:00</published><updated>2018-06-15T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/15/Scan-vs-Query-API-Calls</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/15/Scan-vs-Query-API-Calls.html">&lt;h3 id=&quot;what-is-a-query&quot;&gt;What is a query&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A Query operation finds items in a table using only primary key attributes values. You must provide a partition attribute name and a distinct value to search for.&lt;/li&gt;
  &lt;li&gt;You can optionally provide a sort key attribute name and value, and use a comparison operator to refine the search results.&lt;/li&gt;
  &lt;li&gt;By default, a query returns all of the data attributes for items with the specified primary key(s); however, you can use the &lt;strong&gt;ProjectionExpression&lt;/strong&gt; parameter so that the Query only returns some of the attributes, rather than all of them.&lt;/li&gt;
  &lt;li&gt;Query results are always sorted by the sort key. If the data type of the sort key is a number, the results are returned in numeric order; otherwise the results are returned in order of ASCII character code values. By default, the sort order is ascending. To reverse the order, set the &lt;strong&gt;ScanIndexForward&lt;/strong&gt; parameter to false.&lt;/li&gt;
  &lt;li&gt;By default all queries are eventually consistent but can be changed to be strongly consistent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-a-scan-&quot;&gt;What is a Scan ?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A Scan operation examines every item in the table ( basically a table dump ). By default the scan returns all of the data attributes for every item; however, you can use the &lt;strong&gt;ProjectionExpression&lt;/strong&gt; parameter so that the Scan returns only some of them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-should-i-use-a-query-or-a-scan&quot;&gt;What Should I use, a query or a Scan?&lt;/h3&gt;
&lt;p&gt;Generally, a Query Operation is more efficient than a Scan operation.&lt;/p&gt;

&lt;p&gt;A Scan operation always scans the entire table, then filters out values to provide the desired result, essentially adding the extra step of removing data from the result set. Avoid using a Scan operation on a large table with a filter that removes many results, if possible. Also, as a table grows, the Scan operation slows. The Scan operation examines every item for the requested values, and can use up the provisioned throughput for a large table in a single operation.&lt;/p&gt;

&lt;p&gt;For quicker response times, design your tables in a way that can use the Query, &lt;em&gt;Get&lt;/em&gt; or &lt;em&gt;BatchGetItem&lt;/em&gt; APIs instead. Alternatively, design your application to use Scan operations in a way that minimizes the impact on your tables request rate.&lt;/p&gt;

&lt;h3 id=&quot;exam-tips&quot;&gt;Exam Tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A Query operation finds the items in a table using only primary key attribute values. You must provide a partition key attribute name and a distinct value to search for.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Scan operation examines every item in the table. By default, a Scan returns all of the data attributes for every item; however, you can use the &lt;strong&gt;ProjectionExpression&lt;/strong&gt; parameter so that the Scan only returns some of the attributes, rather than all of them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Query results are always sorted by the sort key in ascending order. Set &lt;strong&gt;ScanIndexForward&lt;/strong&gt; parameter to false to reverse it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Try use a Query operation than a scan operation as it is more efficient.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="scan" /><category term="query" /><category term="dynamodb" /><category term="database" /><summary type="html">What is a query A Query operation finds items in a table using only primary key attributes values. You must provide a partition attribute name and a distinct value to search for. You can optionally provide a sort key attribute name and value, and use a comparison operator to refine the search results. By default, a query returns all of the data attributes for items with the specified primary key(s); however, you can use the ProjectionExpression parameter so that the Query only returns some of the attributes, rather than all of them. Query results are always sorted by the sort key. If the data type of the sort key is a number, the results are returned in numeric order; otherwise the results are returned in order of ASCII character code values. By default, the sort order is ascending. To reverse the order, set the ScanIndexForward parameter to false. By default all queries are eventually consistent but can be changed to be strongly consistent.</summary></entry><entry><title type="html">DynamoDB Indexes</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/15/DynamoDB-Indexes.html" rel="alternate" type="text/html" title="DynamoDB Indexes" /><published>2018-06-15T00:00:00+10:00</published><updated>2018-06-15T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/15/DynamoDB-Indexes</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/15/DynamoDB-Indexes.html">&lt;h3 id=&quot;primary-key&quot;&gt;Primary Key&lt;/h3&gt;
&lt;p&gt;Two types of primary keys available.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Single Attribute (unique ID)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Partition Key (Hash Key) composed of one attribute.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Composite (unique ID and another attribute eg: DateRange)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Partition Key &amp;amp; Sort Key ( Hash &amp;amp; Range ) composed of two attributes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;partition-key&quot;&gt;Partition Key&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DynamoDB uses the partition keys value as input to an internal hash function. The output from the hash function determines the partition ( The physical location where the data will be stored. )&lt;/li&gt;
  &lt;li&gt;No two items can have the same partition key.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;partition-key--sort-key&quot;&gt;Partition Key &amp;amp; Sort Key&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DynamoDB uses the partition keys value as input to the internal hash function. The output determines the physical location just like the previous definition.&lt;/li&gt;
  &lt;li&gt;Two items can have the same partition key, but the sort key must be different.&lt;/li&gt;
  &lt;li&gt;All items with the same partition key will be stored together in sorted order by the sort key.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;local-secondary-index&quot;&gt;Local Secondary Index&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Has the partition key, different sort key.&lt;/li&gt;
  &lt;li&gt;Can &lt;strong&gt;only be created when you create the table.&lt;/strong&gt; They cannot be removed / modified later either.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;global-secondary-index&quot;&gt;Global Secondary Index&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Has a different partition key and a different sort key.&lt;/li&gt;
  &lt;li&gt;Can be created when your creating the table, and it can be created or added later as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamodb-streams&quot;&gt;DynamoDB Streams&lt;/h3&gt;
&lt;p&gt;Used to capture any kind of modification to the DynamoDB tables.
    * If a new item is added to the table, the stream captures an image of the entire item. including all of its attributes.
    * If an item is updated, stream captures the “before” and “after” image of any attributes that were modified in the item.
    * If an item is deleted from the table, the stream captures an image of the entire item before it was deleted.
    * If a new user signed up to your website, it creates a new entry in the user table, this will create a stream if you have streams enabled in your dynamo table. You can use the stream to &lt;strong&gt;trigger&lt;/strong&gt; lambdas to update a dynamodb table in another region or send a welcome message to the user.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You can export data to csv&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;You can also enable cloudwatch to trigger alarms for different different metrics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;You can have upto 5 local secondry indexes (LSI) and 5 global secondary indexes (GSI).&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Eshan Shafeeq</name></author><category term="indexes" /><category term="primarykeys" /><category term="streams" /><category term="dynamodb" /><category term="nosql" /><summary type="html">Primary Key Two types of primary keys available. Single Attribute (unique ID) Partition Key (Hash Key) composed of one attribute. Composite (unique ID and another attribute eg: DateRange) Partition Key &amp;amp; Sort Key ( Hash &amp;amp; Range ) composed of two attributes.</summary></entry><entry><title type="html">Snowball</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/Snowball.html" rel="alternate" type="text/html" title="Snowball" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/Snowball</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/Snowball.html">&lt;p&gt;AWS Import/Export Disk accelerates moving large amounts data into and out of the AWS cloud using portable storage devices for transport. AWS Import/Export Disk transfers your data directly onto and off of storage devices using Amazon’s high-speed internal network and bypassing the internet.&lt;/p&gt;

&lt;h3 id=&quot;types-of-snowballs&quot;&gt;Types of Snowballs&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Snowball&lt;/li&gt;
  &lt;li&gt;Snowball Edge&lt;/li&gt;
  &lt;li&gt;Snowmobile&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;snowball&quot;&gt;Snowball&lt;/h3&gt;
&lt;p&gt;Snowball is petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed internet. 80 TB snowball in all regions. Snowball uses multiple layers of security designed to protect your data including tamper-resistent enclosures, 256-bit encryption, and an industry standard Trusted Platform Module (TPM) designed to ensure both security and full chain-of-custody of your data. Once the data transfer job has been processed and verified, AWS performs a software erasure of Snowball appliance.&lt;/p&gt;

&lt;h3 id=&quot;snowball-edge&quot;&gt;Snowball Edge&lt;/h3&gt;
&lt;p&gt;AWS Snowball Edge is a 100TB data transfer device with on-board storage and compute capabilities. You can use Snowball Edge to move large amounts of data into and out of AWS, as a temporary storage tier for large local datasets, or to support local workloads in remote or offline locations.&lt;/p&gt;

&lt;p&gt;Snowball Edge connects your existing applications and infrastructure using standard storage interfaces, streamlinning the data transfer process and minimizing setup and integration. Snowball Edge can cluster together to form a local storage tier and process your data on-premises, helping ensure your applications continue to run even when they are not able to access the cloud.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Its not just storage capacity its compute capacity in a box as well&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;snowmobile&quot;&gt;Snowmobile&lt;/h3&gt;
&lt;p&gt;AWS Snowmobile is an Exabyte-scale data transfer service use to move extremely large amounts of data to AWS. You can transfer upto 100PB per Snowmobile. a 45-long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even complete data center migration. Transfering data with Snowmobile is secure, fast and cost effective.&lt;/p&gt;

&lt;h3 id=&quot;exam-tips&quot;&gt;Exam tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Understand what snowball is&lt;/li&gt;
  &lt;li&gt;Understand what Import Export is&lt;/li&gt;
  &lt;li&gt;Snowball Can do
    &lt;ul&gt;
      &lt;li&gt;Import to S3&lt;/li&gt;
      &lt;li&gt;Export from S3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="transfer" /><summary type="html">AWS Import/Export Disk accelerates moving large amounts data into and out of the AWS cloud using portable storage devices for transport. AWS Import/Export Disk transfers your data directly onto and off of storage devices using Amazon’s high-speed internal network and bypassing the internet.</summary></entry><entry><title type="html">S3 Transfer Acceleration</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Transfer-Acceleration.html" rel="alternate" type="text/html" title="S3 Transfer Acceleration" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Transfer-Acceleration</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Transfer-Acceleration.html">&lt;p&gt;S3 Transfer Acceleration utilises the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that to S3. You will get a distinct URL to upload to.&lt;/p&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="transfer" /><category term="s3" /><category term="transferacceleration" /><summary type="html">S3 Transfer Acceleration utilises the CloudFront Edge Network to accelerate your uploads to S3. Instead of uploading directly to your S3 bucket, you can use a distinct URL to upload directly to an edge location which will then transfer that to S3. You will get a distinct URL to upload to.</summary></entry><entry><title type="html">S3 Storage Summary</title><link href="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Storage-Summary.html" rel="alternate" type="text/html" title="S3 Storage Summary" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Storage-Summary</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/storage/2018/06/14/S3-Storage-Summary.html">&lt;h3 id=&quot;s3---exam-tips&quot;&gt;S3 - Exam Tips&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;S3 is an Object based storage, its a place where you upload and download files.
    &lt;ul&gt;
      &lt;li&gt;You cannot run an os, or run a database off of S3.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Files can be of size 0 bytes all the way to 5TB.&lt;/li&gt;
  &lt;li&gt;There is unlimited storage.&lt;/li&gt;
  &lt;li&gt;Files are stored in Buckets.&lt;/li&gt;
  &lt;li&gt;S3 is a universal namespace, all bucket names shall be unique globally.&lt;/li&gt;
  &lt;li&gt;S3 bucket names are like &lt;code class=&quot;highlighter-rouge&quot;&gt;https://s3-[region].amazonaws.com/[bucketname]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Read after Write Consistency for PUTS of new objects.&lt;/li&gt;
  &lt;li&gt;Eventual Consistencry for Overwrite PUTS and DELETES. ( takes some time to propergate )&lt;/li&gt;
  &lt;li&gt;S3 Storage Classes / Tiers
    &lt;ul&gt;
      &lt;li&gt;Standard S3
        &lt;ul&gt;
          &lt;li&gt;reliability of 99.999999999%&lt;/li&gt;
          &lt;li&gt;availability of 99.99%&lt;/li&gt;
          &lt;li&gt;Immedietly available and frequently accessed&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;S3 Infrequent Access ( IA )
        &lt;ul&gt;
          &lt;li&gt;availablity of 99.99%&lt;/li&gt;
          &lt;li&gt;reliability of 99.999999999%&lt;/li&gt;
          &lt;li&gt;Immedietly available infrequently accessed.&lt;/li&gt;
          &lt;li&gt;Charged for retreval&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;S3 Reduced Redundency
        &lt;ul&gt;
          &lt;li&gt;reliability and availability of 99.99%&lt;/li&gt;
          &lt;li&gt;Used for reproducable content ( such as thumbnails etc ).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Glacier
        &lt;ul&gt;
          &lt;li&gt;Archived data, where you have to wait 3 - 5 hour waiting period.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Core fundementals of S3
    &lt;ul&gt;
      &lt;li&gt;Key ( name )&lt;/li&gt;
      &lt;li&gt;Value ( data )&lt;/li&gt;
      &lt;li&gt;Version ID&lt;/li&gt;
      &lt;li&gt;Metadata ( data about data )&lt;/li&gt;
      &lt;li&gt;Access control lists&lt;/li&gt;
      &lt;li&gt;Object based stroage only ( flat files )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 versioning
    &lt;ul&gt;
      &lt;li&gt;Stores all versions of an object ( including all writes and even if you delete an object )&lt;/li&gt;
      &lt;li&gt;Great backup tool&lt;/li&gt;
      &lt;li&gt;You pay for storage for all the versions of the object.&lt;/li&gt;
      &lt;li&gt;Once enabled versioning cannot be disabled, it can only be suspended.&lt;/li&gt;
      &lt;li&gt;Integrates with lifecycle rules.&lt;/li&gt;
      &lt;li&gt;Versionings MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.&lt;/li&gt;
      &lt;li&gt;Cross region replication requires versioning enabled on both the source bucket and destination bucket.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Lifecycle Management
    &lt;ul&gt;
      &lt;li&gt;Can be used in conjunction with versioning.&lt;/li&gt;
      &lt;li&gt;Can be applied to current versions and previous versions.&lt;/li&gt;
      &lt;li&gt;Following actions can now be done.
        &lt;ul&gt;
          &lt;li&gt;Transition to infrequenct access after a minimum of 30 days since creation date ( files need to be minimum 128kb )&lt;/li&gt;
          &lt;li&gt;Archived to glacier storage class ( 30 days after IA, if relevant ).&lt;/li&gt;
          &lt;li&gt;Permenantly Delete Objects&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CloudFront
    &lt;ul&gt;
      &lt;li&gt;Edge Locations - This is the location where content will be cached.&lt;/li&gt;
      &lt;li&gt;Origin - This is the origin of all the files that the CDN will distribute. This can be either an S3 bucket, an EC2 instance, an Elastic Load balancer or Route53.&lt;/li&gt;
      &lt;li&gt;Distribution - This is the name given to the CDN with a collections of Edge Locations.
        &lt;ul&gt;
          &lt;li&gt;Web Distribution - Typically used for websites&lt;/li&gt;
          &lt;li&gt;RTMP - Used for Media Streaming&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Edge locations are not just readonly, you can write to them as well. ( Upload files to S3 ).&lt;/li&gt;
      &lt;li&gt;Object are cached for the life of the TTL ( time to live ) ( default ttl 24hours )&lt;/li&gt;
      &lt;li&gt;You can clear cached objects, but you will be charged.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Securing your buckets.
    &lt;ul&gt;
      &lt;li&gt;By default, all newly created buckets are &lt;strong&gt;PRIVATE&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;You can setup access control to your buckets using;
        &lt;ul&gt;
          &lt;li&gt;Bucket policies.&lt;/li&gt;
          &lt;li&gt;Access Control Lists.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;S3 buckets can be configured to create access logs which logs all the access requests made to the bucke. These logs can be stored in another bucket.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encryption
    &lt;ul&gt;
      &lt;li&gt;In-transit Encryption
        &lt;ul&gt;
          &lt;li&gt;SSL/TLS (HTTPS)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;In rest
        &lt;ul&gt;
          &lt;li&gt;Server Side Encryption
            &lt;ul&gt;
              &lt;li&gt;S3 Managed Keys SSE-S3&lt;/li&gt;
              &lt;li&gt;AWS Managed Keys SSE-KMS
                &lt;ul&gt;
                  &lt;li&gt;Allows you to monitor who decrypted which file when. (Audit trail)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Customer provided Keys SSE-C&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Client Side Encryption
            &lt;ul&gt;
              &lt;li&gt;Client uploads encrypted files.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Storage Gateway
    &lt;ul&gt;
      &lt;li&gt;File Gateway - for flatfiles,  stored directly on S3.&lt;/li&gt;
      &lt;li&gt;Volume Gateway (ISCI) - block based storage
        &lt;ul&gt;
          &lt;li&gt;Stored Volumes - your on premises data is asynchronously backed up to S3.&lt;/li&gt;
          &lt;li&gt;Cached Volumes - frequently accessed data is cached on premises from S3.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Gateway VIrtual Taped Library (VTL)
        &lt;ul&gt;
          &lt;li&gt;Used for backing up your data, from application like Netbackup, Backup Exec, Veam etc.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Snowball
    &lt;ul&gt;
      &lt;li&gt;Snowball - Pure storage 50TB to 80TB&lt;/li&gt;
      &lt;li&gt;Snowball Edge - Both Compute and Storage Capabilities&lt;/li&gt;
      &lt;li&gt;Snowmobile - 100PB worth of storage&lt;/li&gt;
      &lt;li&gt;Understand what snowball is&lt;/li&gt;
      &lt;li&gt;Understand what Import/Export is&lt;/li&gt;
      &lt;li&gt;Snowball can import / Export to S3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 Transfer Acceleration
    &lt;ul&gt;
      &lt;li&gt;You can speed up S3 transfer speeds using CloudFront Edge locations, so you can upload files to s3 faster. Extra costs will apply. Greatest Impact on people who are further away geographically.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;S3 static websites.
    &lt;ul&gt;
      &lt;li&gt;You can S3 to host static websites.&lt;/li&gt;
      &lt;li&gt;Serverless&lt;/li&gt;
      &lt;li&gt;Very Cheap, scales automatically.&lt;/li&gt;
      &lt;li&gt;Static only, cannot host dynamic sites.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CORS
    &lt;ul&gt;
      &lt;li&gt;Cross origin resource sharing&lt;/li&gt;
      &lt;li&gt;Needed to enable cors on the resources bucket and state the URL for the origin that will be calling the bucket.&lt;/li&gt;
      &lt;li&gt;Remeber to use the S3 website url and not the bucket URL, website url has the word website on it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When you do a PUT object on S3 you will get a HTTP 200 success code.&lt;/li&gt;
  &lt;li&gt;You can load files much faster by enabling multipart upload.&lt;/li&gt;
  &lt;li&gt;Read the S3 FAQ!&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="storage" /><category term="s3" /><category term="summary" /><summary type="html">S3 - Exam Tips S3 is an Object based storage, its a place where you upload and download files. You cannot run an os, or run a database off of S3. Files can be of size 0 bytes all the way to 5TB. There is unlimited storage. Files are stored in Buckets. S3 is a universal namespace, all bucket names shall be unique globally. S3 bucket names are like https://s3-[region].amazonaws.com/[bucketname] Read after Write Consistency for PUTS of new objects. Eventual Consistencry for Overwrite PUTS and DELETES. ( takes some time to propergate ) S3 Storage Classes / Tiers Standard S3 reliability of 99.999999999% availability of 99.99% Immedietly available and frequently accessed S3 Infrequent Access ( IA ) availablity of 99.99% reliability of 99.999999999% Immedietly available infrequently accessed. Charged for retreval S3 Reduced Redundency reliability and availability of 99.99% Used for reproducable content ( such as thumbnails etc ). Glacier Archived data, where you have to wait 3 - 5 hour waiting period. Core fundementals of S3 Key ( name ) Value ( data ) Version ID Metadata ( data about data ) Access control lists Object based stroage only ( flat files ) S3 versioning Stores all versions of an object ( including all writes and even if you delete an object ) Great backup tool You pay for storage for all the versions of the object. Once enabled versioning cannot be disabled, it can only be suspended. Integrates with lifecycle rules. Versionings MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security. Cross region replication requires versioning enabled on both the source bucket and destination bucket. Lifecycle Management Can be used in conjunction with versioning. Can be applied to current versions and previous versions. Following actions can now be done. Transition to infrequenct access after a minimum of 30 days since creation date ( files need to be minimum 128kb ) Archived to glacier storage class ( 30 days after IA, if relevant ). Permenantly Delete Objects CloudFront Edge Locations - This is the location where content will be cached. Origin - This is the origin of all the files that the CDN will distribute. This can be either an S3 bucket, an EC2 instance, an Elastic Load balancer or Route53. Distribution - This is the name given to the CDN with a collections of Edge Locations. Web Distribution - Typically used for websites RTMP - Used for Media Streaming Edge locations are not just readonly, you can write to them as well. ( Upload files to S3 ). Object are cached for the life of the TTL ( time to live ) ( default ttl 24hours ) You can clear cached objects, but you will be charged. Securing your buckets. By default, all newly created buckets are PRIVATE. You can setup access control to your buckets using; Bucket policies. Access Control Lists. S3 buckets can be configured to create access logs which logs all the access requests made to the bucke. These logs can be stored in another bucket. Encryption In-transit Encryption SSL/TLS (HTTPS) In rest Server Side Encryption S3 Managed Keys SSE-S3 AWS Managed Keys SSE-KMS Allows you to monitor who decrypted which file when. (Audit trail) Customer provided Keys SSE-C Client Side Encryption Client uploads encrypted files.</summary></entry><entry><title type="html">DynamoDB Introduction</title><link href="https://diehard073055.github.io//sturdy-carnival/database/2018/06/14/DynamoDB-Introduction.html" rel="alternate" type="text/html" title="DynamoDB Introduction" /><published>2018-06-14T00:00:00+10:00</published><updated>2018-06-14T00:00:00+10:00</updated><id>https://diehard073055.github.io//sturdy-carnival/database/2018/06/14/DynamoDB-Introduction</id><content type="html" xml:base="https://diehard073055.github.io//sturdy-carnival/database/2018/06/14/DynamoDB-Introduction.html">&lt;h3 id=&quot;dynamodb-101&quot;&gt;DynamoDB 101&lt;/h3&gt;
&lt;p&gt;Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT, and many other applications.&lt;/p&gt;

&lt;h3 id=&quot;quick-facts-about-dynamodb-&quot;&gt;Quick Facts about DynamoDB ?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Stored on SSD Storage&lt;/li&gt;
  &lt;li&gt;Spread Across 3 geographically distinct data centres.&lt;/li&gt;
  &lt;li&gt;Eventual Consistent Reads (Defaults)
    &lt;ul&gt;
      &lt;li&gt;Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Strongly Consistent Reads
    &lt;ul&gt;
      &lt;li&gt;A strongly consistent read returns a result that reflects all writes that recieved a successful response prior to the read.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-basics&quot;&gt;The Basics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Tables&lt;/li&gt;
  &lt;li&gt;Items ( Think a row of data in table )&lt;/li&gt;
  &lt;li&gt;Attributes ( Think a column of data in a table )&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;DynamoDB can have 35 levels of nesting currently.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;pricing&quot;&gt;Pricing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Provisioned Throughput Capacity
    &lt;ul&gt;
      &lt;li&gt;Write throughput $0.0065 per hour for every 10 units&lt;/li&gt;
      &lt;li&gt;Read throughput $0.0065 per hour for every 50 units&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;First 25 GB stored per month is free.&lt;/li&gt;
  &lt;li&gt;Storage costs of $0.25 per month there after.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets assume that your application needs to perform 1 million writes and 1 million reads per day, while storing 28GB of data. First, you need to calculate how many writes and reads per second you need. 1 million evenly spread writes per day is equivalent to 1,000,000 ( writes ) / 24 ( hours ) / 60 ( minutes ) / 60 ( seconds ) = 11.6 writes per second. A DynamoDB write capacity unit can handle 1 write per second, so you need 12 write capacity units. For write throughput you are charged $0.0065 for every 10 units.
So ($0.0065/10) * 12 * 24 = $0.1872 per day.&lt;/p&gt;

&lt;p&gt;Similarly, to handle 1 million strongly consistent reads per day, you need 12 Read Capacity Units. For read throughput you are charged $0.0065 for every 50 units.
So ( $0.0065/50 ) * 12 * 24 = $0.0374 per day.&lt;/p&gt;

&lt;p&gt;Storage costs is 0.25 per GB per month. Lets assume our database is 28GB. We get the first 25GB for free so we only pay for 3GB of storage which is $0.75 per month.&lt;/p&gt;

&lt;p&gt;Total Cost = $0.1872 per day + $0.0374 per day Plus Storage of 0.75 per month
( 30 x ( $0.1872 + $0.0374 )) $0.75 = $7.488&lt;/p&gt;

&lt;p&gt;With Free Tier You Get
25 Read Capacity Units
25 Write Capacity Units&lt;/p&gt;

&lt;h3 id=&quot;easiest-way-to-learn-dynamodb&quot;&gt;Easiest Way to Learn DynamoDB&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Play with DynamoDB&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Eshan Shafeeq</name></author><category term="introduction" /><category term="dynamodb" /><category term="nosql" /><category term="database" /><summary type="html">DynamoDB 101 Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, ad-tech, IoT, and many other applications.</summary></entry></feed>